{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z23fInsrJVA_"
   },
   "source": [
    "### Module Import & Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qDv2R7qDJBOo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "J5qEqcUbJBPX"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(777)  # 랜덤 시드 고정\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777) # GPU 사용 가능할경우 랜덤 시드 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rhaabBsLtNR"
   },
   "source": [
    "# Activation Function 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZS0oCzPLbBP"
   },
   "source": [
    "### Abstract Class for each Activation Funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rWEkLTiGJBPv"
   },
   "outputs": [],
   "source": [
    "class BaseActivation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bebvInNcJBQ3"
   },
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "90FIOJiWJBQ6"
   },
   "outputs": [],
   "source": [
    "# Hint: torch.exp함수를 사용해보세요.\n",
    "# Hint: https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html\n",
    "\n",
    "class sigmoid(BaseActivation):\n",
    "    def __call__(self, x):\n",
    "        ################ TODO ################\n",
    "        ret = 1/(1+torch.exp(-x))\n",
    "        return ret\n",
    "        ################ TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiGtIqC_JBQj"
   },
   "source": [
    "### ReLU\n",
    "Relu(x) = max(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FbawAVTEJBQP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "zero_tensor = torch.Tensor([0]).to(device)\n",
    "print(zero_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qknNNXUHJBQn"
   },
   "outputs": [],
   "source": [
    "# Hint: torch.maximum과 zero_tensor를 활용해보세요.\n",
    "# Hint: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU\n",
    "class relu(BaseActivation):\n",
    "    def __call__(self, x):\n",
    "        ################ TODO ################\n",
    "        ret = torch.maximum(x,zero_tensor)\n",
    "        return ret\n",
    "        ################ TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lO6flsUSJBRE"
   },
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "erEn1u70JBRG"
   },
   "outputs": [],
   "source": [
    "# Hint 1: torch.Tensor를 활용해 negative slope를 정의하세요\n",
    "# Hint 2: torch.where를 활용해 조건에 따른 return값을 설정해보세요\n",
    "# Hint: https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html?highlight=leakyrelu\n",
    "\n",
    "class leakyrelu(BaseActivation):\n",
    "    def __init__(self):\n",
    "        ################ TODO ################\n",
    "        self.negative_slope = torch.Tensor([1e-2]).to(device)\n",
    "        ################ TODO ################\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        ################ TODO ################\n",
    "        ret = torch.where(x>0,x,self.negative_slope*x)\n",
    "        return ret\n",
    "        ################ TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3zrXoF2JBRJ"
   },
   "source": [
    "### PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5dp188PQJBRL"
   },
   "outputs": [],
   "source": [
    "# Hint: 위와 마찬가지로 조건에 따른 return값을 설정해보세요.\n",
    "# Hint: https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html?highlight=prelu#torch.nn.PReLU\n",
    "\n",
    "class prelu(BaseActivation):\n",
    "    def __init__(self):\n",
    "        self.alpha = torch.empty(1).fill_(0.25).to(device).requires_grad_()\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        ################ TODO ################\n",
    "        ret = torch.where(x>=0, x, self.alpha*x)\n",
    "        return ret\n",
    "        ################ TODO ################\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.alpha]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-tDsQ8nJBRU"
   },
   "source": [
    "### ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Zfl1mD32JBRY"
   },
   "outputs": [],
   "source": [
    "# Hint: https://pytorch.org/docs/stable/generated/torch.nn.ELU.html?highlight=elu#torch.nn.ELU\n",
    "\n",
    "class elu(BaseActivation):\n",
    "    def __init__(self):\n",
    "        self.alpha = torch.empty(1).fill_(1.0).to(device).requires_grad_()\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        ################ TODO ################\n",
    "        ret = torch.where(x>0,x,self.alpha*(torch.exp(x)-1))\n",
    "        return ret\n",
    "        ################ TODO ################\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.alpha]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofZduPjZMG68"
   },
   "source": [
    "###Your Custom Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uwmfE8HYJBRi"
   },
   "outputs": [],
   "source": [
    "# Custom activation을 만들어보세요.\n",
    "class my_activation_1(BaseActivation):\n",
    "    def __init__(self):\n",
    "        self.alpha=1.01\n",
    "        self.negative_slope =0.01\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        ################ TODO ################\n",
    "        return torch.where(x>=0, self.alpha*x,self.negative_slope*x)\n",
    "        ################ TODO ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom activation을 만들어보세요.\n",
    "class my_activation_2(BaseActivation):\n",
    "    def __init__(self):\n",
    "        self.alpha =1.00\n",
    "        self.negative_slope = 0.1\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        ################ TODO ################\n",
    "        return torch.where(x>=0, self.alpha*x, self.negative_slope*torch.exp(x))\n",
    "        ################ TODO ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom activation을 만들어보세요.\n",
    "class my_activation_3(BaseActivation):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        ################ TODO ################\n",
    "        return torch.mul(x,torch.div(1,torch.add(1,torch.exp(torch.negative(-x)))))\n",
    "        ################ TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZ6IpM7PMNkS"
   },
   "source": [
    "# MNIST 손글씨 이미지 분류하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhcqNwS4JrLF"
   },
   "source": [
    "### 하이퍼파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "N-Ig8-UCJBPe"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atH1wDx6J4P4"
   },
   "source": [
    "### 데이터셋, 데이터 로더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vfp3nggsJBPh",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jk121925\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "#손글씨 분류를 위한 데이터셋인 MNIST 데이터셋 다운로드\n",
    "\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/', # 다운로드 경로 지정\n",
    "                          train=True, # True로 지정하면 훈련 데이터로 다운로드\n",
    "                          transform=transforms.ToTensor(), # 텐서 데이터로 변환\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/', # 다운로드 경로 지정\n",
    "                         train=False, # False로 지정하면 테스트 데이터로 다운로드\n",
    "                         transform=transforms.ToTensor(), # 텐서 데이터로 변환\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eHvC3MFAJBPm"
   },
   "outputs": [],
   "source": [
    "# 다운로드 한 데이터셋을 이용해 데이터 로더 정의\n",
    "# 데이터 로더는 iterable type으로, 반복문을 사용하여 한 번에 배치 사이즈만큼 데이터를 불러올 수 있음\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "total_batch = len(data_loader)  # Mini-batch의 개수 = (전체 데이터 수 / batch size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaTzesIsOPkE"
   },
   "source": [
    "### 모델(신경망) 구조 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4lxQFEJcJBRl"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, use):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(28*28, 64) # input image shape가 가로 28, 세로 28이므로 28*28\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 16)\n",
    "        self.fc4 = torch.nn.Linear(16, 10) # 최종 예측 클래스 종류는 0~9, 10가지이므로 10개\n",
    "        \n",
    "        # 파라미터로 주어진 activation 종다르게 설정\n",
    "        if use == 'relu':\n",
    "            self.activation = relu()\n",
    "        elif use == 'sigmoid':\n",
    "            self.activation = sigmoid()\n",
    "        elif use == 'prelu':\n",
    "            self.activation = prelu()\n",
    "        elif use == 'leaky_relu':\n",
    "            self.activation = leakyrelu()\n",
    "        elif use == 'elu':\n",
    "            self.activation = elu()\n",
    "        elif use == 'custom_1':\n",
    "            self.activation = my_activation_1()\n",
    "        elif use == 'custom_2':\n",
    "            self.activation = my_activation_2()\n",
    "        elif use == 'custom_3':\n",
    "            self.activation = my_activation_3()\n",
    "            \n",
    "    # fc1 -> activation -> fc2\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HypBuqJwOgxm"
   },
   "source": [
    "### 모델 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ssB9q5IXJBRy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 모델을 생성하고 학습시킨 후, 성능을 performance_dict에 저장하는 함수\n",
    "def train_eval(use):\n",
    "    global performance_dict\n",
    "    model = Net(use).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + model.activation.parameters(), lr=learning_rate)\n",
    "\n",
    "    start = time.time(); test_accs = []\n",
    "    print(f\"\\n###### Using Activation Function: {use.upper()} ######\")\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = 0\n",
    "        avg_acc = 0\n",
    "\n",
    "        for X, Y in data_loader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(X)\n",
    "            loss = criterion(prediction, Y)\n",
    "            acc = (torch.argmax(prediction, 1) == Y).float().mean().item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss / total_batch\n",
    "            avg_acc += acc / total_batch\n",
    "\n",
    "        print(f'[Epoch: {epoch+1:>2}] train_loss: {avg_loss:.4f}, train_acc: {avg_acc:.4f}, ', end=\"\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X_test = mnist_test.data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
    "            Y_test = mnist_test.targets.to(device)\n",
    "\n",
    "            test_prediction = model(X_test)\n",
    "            correct_prediction = torch.argmax(test_prediction, 1) == Y_test\n",
    "            test_accuracy = correct_prediction.float().mean().item()\n",
    "            test_accs.append(test_accuracy)\n",
    "            print(f\"test_acc: {test_accuracy:.4f}, best_acc: {max(test_accs):.4f}\")\n",
    "    \n",
    "    performance_dict[use] = {'time': time.time()-start, 'accuracy' : max(test_accs)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWh38u5AOmSC"
   },
   "source": [
    "### 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TMxze1x9JBR7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###### Using Activation Function: RELU ######\n",
      "[Epoch:  1] train_loss: 0.8153, train_acc: 0.7293, test_acc: 0.9053, best_acc: 0.9053\n",
      "[Epoch:  2] train_loss: 0.2387, train_acc: 0.9306, test_acc: 0.9416, best_acc: 0.9416\n",
      "[Epoch:  3] train_loss: 0.1682, train_acc: 0.9501, test_acc: 0.9548, best_acc: 0.9548\n",
      "[Epoch:  4] train_loss: 0.1317, train_acc: 0.9600, test_acc: 0.9496, best_acc: 0.9548\n",
      "[Epoch:  5] train_loss: 0.1042, train_acc: 0.9693, test_acc: 0.9622, best_acc: 0.9622\n",
      "[Epoch:  6] train_loss: 0.0889, train_acc: 0.9728, test_acc: 0.9662, best_acc: 0.9662\n",
      "[Epoch:  7] train_loss: 0.0782, train_acc: 0.9756, test_acc: 0.9622, best_acc: 0.9662\n",
      "[Epoch:  8] train_loss: 0.0691, train_acc: 0.9785, test_acc: 0.9689, best_acc: 0.9689\n",
      "[Epoch:  9] train_loss: 0.0624, train_acc: 0.9804, test_acc: 0.9681, best_acc: 0.9689\n",
      "[Epoch: 10] train_loss: 0.0530, train_acc: 0.9832, test_acc: 0.9667, best_acc: 0.9689\n",
      "\n",
      "###### Using Activation Function: SIGMOID ######\n",
      "[Epoch:  1] train_loss: 1.7322, train_acc: 0.4706, test_acc: 0.8528, best_acc: 0.8528\n",
      "[Epoch:  2] train_loss: 0.5406, train_acc: 0.8907, test_acc: 0.9235, best_acc: 0.9235\n",
      "[Epoch:  3] train_loss: 0.2738, train_acc: 0.9368, test_acc: 0.9306, best_acc: 0.9306\n",
      "[Epoch:  4] train_loss: 0.1931, train_acc: 0.9530, test_acc: 0.9378, best_acc: 0.9378\n",
      "[Epoch:  5] train_loss: 0.1489, train_acc: 0.9633, test_acc: 0.9455, best_acc: 0.9455\n",
      "[Epoch:  6] train_loss: 0.1220, train_acc: 0.9688, test_acc: 0.9539, best_acc: 0.9539\n",
      "[Epoch:  7] train_loss: 0.1008, train_acc: 0.9745, test_acc: 0.9522, best_acc: 0.9539\n",
      "[Epoch:  8] train_loss: 0.0882, train_acc: 0.9777, test_acc: 0.9543, best_acc: 0.9543\n",
      "[Epoch:  9] train_loss: 0.0751, train_acc: 0.9801, test_acc: 0.9557, best_acc: 0.9557\n",
      "[Epoch: 10] train_loss: 0.0645, train_acc: 0.9837, test_acc: 0.9513, best_acc: 0.9557\n",
      "\n",
      "###### Using Activation Function: PRELU ######\n",
      "[Epoch:  1] train_loss: 0.6319, train_acc: 0.8082, test_acc: 0.9020, best_acc: 0.9020\n",
      "[Epoch:  2] train_loss: 0.2423, train_acc: 0.9289, test_acc: 0.9325, best_acc: 0.9325\n",
      "[Epoch:  3] train_loss: 0.1663, train_acc: 0.9505, test_acc: 0.9495, best_acc: 0.9495\n",
      "[Epoch:  4] train_loss: 0.1238, train_acc: 0.9628, test_acc: 0.9598, best_acc: 0.9598\n",
      "[Epoch:  5] train_loss: 0.1010, train_acc: 0.9699, test_acc: 0.9584, best_acc: 0.9598\n",
      "[Epoch:  6] train_loss: 0.0827, train_acc: 0.9743, test_acc: 0.9588, best_acc: 0.9598\n",
      "[Epoch:  7] train_loss: 0.0691, train_acc: 0.9787, test_acc: 0.9658, best_acc: 0.9658\n",
      "[Epoch:  8] train_loss: 0.0602, train_acc: 0.9814, test_acc: 0.9682, best_acc: 0.9682\n",
      "[Epoch:  9] train_loss: 0.0513, train_acc: 0.9841, test_acc: 0.9675, best_acc: 0.9682\n",
      "[Epoch: 10] train_loss: 0.0446, train_acc: 0.9857, test_acc: 0.9644, best_acc: 0.9682\n",
      "\n",
      "###### Using Activation Function: LEAKY_RELU ######\n",
      "[Epoch:  1] train_loss: 0.7548, train_acc: 0.7498, test_acc: 0.9212, best_acc: 0.9212\n",
      "[Epoch:  2] train_loss: 0.2117, train_acc: 0.9387, test_acc: 0.9463, best_acc: 0.9463\n",
      "[Epoch:  3] train_loss: 0.1458, train_acc: 0.9572, test_acc: 0.9605, best_acc: 0.9605\n",
      "[Epoch:  4] train_loss: 0.1116, train_acc: 0.9665, test_acc: 0.9617, best_acc: 0.9617\n",
      "[Epoch:  5] train_loss: 0.0892, train_acc: 0.9736, test_acc: 0.9640, best_acc: 0.9640\n",
      "[Epoch:  6] train_loss: 0.0752, train_acc: 0.9773, test_acc: 0.9681, best_acc: 0.9681\n",
      "[Epoch:  7] train_loss: 0.0737, train_acc: 0.9768, test_acc: 0.9680, best_acc: 0.9681\n",
      "[Epoch:  8] train_loss: 0.0576, train_acc: 0.9813, test_acc: 0.9705, best_acc: 0.9705\n",
      "[Epoch:  9] train_loss: 0.0534, train_acc: 0.9825, test_acc: 0.9673, best_acc: 0.9705\n",
      "[Epoch: 10] train_loss: 0.0459, train_acc: 0.9851, test_acc: 0.9659, best_acc: 0.9705\n",
      "\n",
      "###### Using Activation Function: ELU ######\n",
      "[Epoch:  1] train_loss: 0.5630, train_acc: 0.8277, test_acc: 0.8911, best_acc: 0.8911\n",
      "[Epoch:  2] train_loss: 0.1959, train_acc: 0.9417, test_acc: 0.9186, best_acc: 0.9186\n",
      "[Epoch:  3] train_loss: 0.1329, train_acc: 0.9597, test_acc: 0.9088, best_acc: 0.9186\n",
      "[Epoch:  4] train_loss: 0.1046, train_acc: 0.9680, test_acc: 0.8952, best_acc: 0.9186\n",
      "[Epoch:  5] train_loss: 0.0883, train_acc: 0.9725, test_acc: 0.9288, best_acc: 0.9288\n",
      "[Epoch:  6] train_loss: 0.0722, train_acc: 0.9773, test_acc: 0.8844, best_acc: 0.9288\n",
      "[Epoch:  7] train_loss: 0.0620, train_acc: 0.9801, test_acc: 0.8853, best_acc: 0.9288\n",
      "[Epoch:  8] train_loss: 0.0534, train_acc: 0.9829, test_acc: 0.9012, best_acc: 0.9288\n",
      "[Epoch:  9] train_loss: 0.0449, train_acc: 0.9854, test_acc: 0.8826, best_acc: 0.9288\n",
      "[Epoch: 10] train_loss: 0.0417, train_acc: 0.9869, test_acc: 0.8913, best_acc: 0.9288\n",
      "\n",
      "###### Using Activation Function: CUSTOM_1 ######\n",
      "[Epoch:  1] train_loss: 0.7011, train_acc: 0.7798, test_acc: 0.9210, best_acc: 0.9210\n",
      "[Epoch:  2] train_loss: 0.2240, train_acc: 0.9337, test_acc: 0.9446, best_acc: 0.9446\n",
      "[Epoch:  3] train_loss: 0.1474, train_acc: 0.9570, test_acc: 0.9572, best_acc: 0.9572\n",
      "[Epoch:  4] train_loss: 0.1181, train_acc: 0.9649, test_acc: 0.9602, best_acc: 0.9602\n",
      "[Epoch:  5] train_loss: 0.1004, train_acc: 0.9691, test_acc: 0.9617, best_acc: 0.9617\n",
      "[Epoch:  6] train_loss: 0.0873, train_acc: 0.9727, test_acc: 0.9638, best_acc: 0.9638\n",
      "[Epoch:  7] train_loss: 0.0717, train_acc: 0.9775, test_acc: 0.9667, best_acc: 0.9667\n",
      "[Epoch:  8] train_loss: 0.0668, train_acc: 0.9790, test_acc: 0.9622, best_acc: 0.9667\n",
      "[Epoch:  9] train_loss: 0.0584, train_acc: 0.9816, test_acc: 0.9671, best_acc: 0.9671\n",
      "[Epoch: 10] train_loss: 0.0528, train_acc: 0.9833, test_acc: 0.9623, best_acc: 0.9671\n",
      "\n",
      "###### Using Activation Function: CUSTOM_2 ######\n",
      "[Epoch:  1] train_loss: 0.6826, train_acc: 0.7857, test_acc: 0.9188, best_acc: 0.9188\n",
      "[Epoch:  2] train_loss: 0.2233, train_acc: 0.9345, test_acc: 0.9443, best_acc: 0.9443\n",
      "[Epoch:  3] train_loss: 0.1528, train_acc: 0.9546, test_acc: 0.9536, best_acc: 0.9536\n",
      "[Epoch:  4] train_loss: 0.1201, train_acc: 0.9636, test_acc: 0.9631, best_acc: 0.9631\n",
      "[Epoch:  5] train_loss: 0.1014, train_acc: 0.9695, test_acc: 0.9589, best_acc: 0.9631\n",
      "[Epoch:  6] train_loss: 0.0829, train_acc: 0.9748, test_acc: 0.9660, best_acc: 0.9660\n",
      "[Epoch:  7] train_loss: 0.0734, train_acc: 0.9773, test_acc: 0.9696, best_acc: 0.9696\n",
      "[Epoch:  8] train_loss: 0.0606, train_acc: 0.9808, test_acc: 0.9678, best_acc: 0.9696\n",
      "[Epoch:  9] train_loss: 0.0520, train_acc: 0.9837, test_acc: 0.9688, best_acc: 0.9696\n",
      "[Epoch: 10] train_loss: 0.0492, train_acc: 0.9842, test_acc: 0.9710, best_acc: 0.9710\n",
      "\n",
      "###### Using Activation Function: CUSTOM_3 ######\n",
      "[Epoch:  1] train_loss: 0.6451, train_acc: 0.7966, test_acc: 0.9255, best_acc: 0.9255\n",
      "[Epoch:  2] train_loss: 0.1927, train_acc: 0.9433, test_acc: 0.9472, best_acc: 0.9472\n",
      "[Epoch:  3] train_loss: 0.1299, train_acc: 0.9612, test_acc: 0.9598, best_acc: 0.9598\n",
      "[Epoch:  4] train_loss: 0.0980, train_acc: 0.9702, test_acc: 0.9628, best_acc: 0.9628\n",
      "[Epoch:  5] train_loss: 0.0837, train_acc: 0.9745, test_acc: 0.9676, best_acc: 0.9676\n",
      "[Epoch:  6] train_loss: 0.0678, train_acc: 0.9784, test_acc: 0.9682, best_acc: 0.9682\n",
      "[Epoch:  7] train_loss: 0.0566, train_acc: 0.9824, test_acc: 0.9675, best_acc: 0.9682\n",
      "[Epoch:  8] train_loss: 0.0523, train_acc: 0.9829, test_acc: 0.9634, best_acc: 0.9682\n",
      "[Epoch:  9] train_loss: 0.0450, train_acc: 0.9852, test_acc: 0.9671, best_acc: 0.9682\n",
      "[Epoch: 10] train_loss: 0.0424, train_acc: 0.9859, test_acc: 0.9636, best_acc: 0.9682\n"
     ]
    }
   ],
   "source": [
    "performance_dict = {}\n",
    "for use in ['relu','sigmoid','prelu', 'leaky_relu', 'elu','custom_1','custom_2','custom_3']:   #위에서 정의한 각 활성화함수를 이용해 훈련\n",
    "    train_eval(use)\n",
    "performance_df = pd.DataFrame(performance_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tbbYtSWlJBR-"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>relu</th>\n",
       "      <td>48.611004</td>\n",
       "      <td>0.9689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigmoid</th>\n",
       "      <td>44.401778</td>\n",
       "      <td>0.9557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prelu</th>\n",
       "      <td>45.749871</td>\n",
       "      <td>0.9682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leaky_relu</th>\n",
       "      <td>46.419384</td>\n",
       "      <td>0.9705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elu</th>\n",
       "      <td>44.519744</td>\n",
       "      <td>0.9288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>custom_1</th>\n",
       "      <td>44.594411</td>\n",
       "      <td>0.9671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>custom_2</th>\n",
       "      <td>44.576229</td>\n",
       "      <td>0.9710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>custom_3</th>\n",
       "      <td>44.784685</td>\n",
       "      <td>0.9682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time  accuracy\n",
       "relu        48.611004    0.9689\n",
       "sigmoid     44.401778    0.9557\n",
       "prelu       45.749871    0.9682\n",
       "leaky_relu  46.419384    0.9705\n",
       "elu         44.519744    0.9288\n",
       "custom_1    44.594411    0.9671\n",
       "custom_2    44.576229    0.9710\n",
       "custom_3    44.784685    0.9682"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bblyiEm_JBSA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Activation Function was: 0.9710, custom_2\n",
      "Custom Activation Function Accuracy Rank: 6\n",
      "Custom Activation Function Accuracy Rank: 1\n",
      "Custom Activation Function Accuracy Rank: 5\n",
      "Custom Activation Function Time Complexity Rank: 4\n",
      "Custom Activation Function Time Complexity Rank: 3\n",
      "Custom Activation Function Time Complexity Rank: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Accuracy Activation Function was: {performance_df['accuracy'].max():.4f}, {performance_df.index[performance_df['accuracy'].argmax()]}\")\n",
    "print(f\"Custom Activation Function Accuracy Rank: {int(performance_df['accuracy'].rank(method='max', ascending=False)['custom_1'])}\")\n",
    "print(f\"Custom Activation Function Accuracy Rank: {int(performance_df['accuracy'].rank(method='max', ascending=False)['custom_2'])}\")\n",
    "print(f\"Custom Activation Function Accuracy Rank: {int(performance_df['accuracy'].rank(method='max', ascending=False)['custom_3'])}\")\n",
    "print(f\"Custom Activation Function Time Complexity Rank: {int(performance_df['time'].rank(method='min')['custom_1'])}\")\n",
    "print(f\"Custom Activation Function Time Complexity Rank: {int(performance_df['time'].rank(method='min')['custom_2'])}\")\n",
    "print(f\"Custom Activation Function Time Complexity Rank: {int(performance_df['time'].rank(method='min')['custom_3'])}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.title(\"Activation Function Compare Plot\")\n",
    "sns.scatterplot(data=performance_df, x='time', y='accuracy', hue=performance_df.index, s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "YAI_2주차 과제_activation function(사본 만들어 사용해주세요).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
